{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 360-degree 3D Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import mmcv\n",
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "from mmcv import Config, DictAction\n",
    "from mmcv.cnn import fuse_conv_bn\n",
    "from mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n",
    "from mmcv.runner import (get_dist_info, init_dist, load_checkpoint,\n",
    "                         wrap_fp16_model)\n",
    "\n",
    "from mmdet3d.apis import single_gpu_test\n",
    "from mmdet3d.datasets import build_dataset\n",
    "from projects.mmdet3d_plugin.datasets.builder import build_dataloader\n",
    "from mmdet3d.models import build_model\n",
    "from mmdet.apis import set_random_seed\n",
    "from projects.mmdet3d_plugin.bevformer.apis.test import custom_multi_gpu_test\n",
    "from mmdet.datasets import replace_ImageToTensor\n",
    "from mmdet3d.datasets.pipelines import Compose\n",
    "from mmdet3d.core.bbox import get_box_type\n",
    "\n",
    "import time\n",
    "import os.path as osp\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:79: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:79: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/tmp/ipykernel_2680571/444686650.py:79: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if device is not 'cpu':\n"
     ]
    }
   ],
   "source": [
    "def init_model(config, checkpoint, device='cuda:0'):\n",
    "    cfg = Config.fromfile(config)\n",
    "\n",
    "    # import modules from string list.\n",
    "    if cfg.get('custom_imports', None):\n",
    "        print(\"custom import\")\n",
    "        from mmcv.utils import import_modules_from_strings\n",
    "        import_modules_from_strings(**cfg['custom_imports'])\n",
    "\n",
    "    # import modules from plguin/xx, registry will be updated\n",
    "    if hasattr(cfg, 'plugin'):\n",
    "        print(\"plugin import\")\n",
    "        if cfg.plugin:\n",
    "            import importlib\n",
    "            if hasattr(cfg, 'plugin_dir'):\n",
    "                plugin_dir = cfg.plugin_dir\n",
    "                _module_dir = os.path.dirname(plugin_dir)\n",
    "                _module_dir = _module_dir.split('/')\n",
    "                _module_path = _module_dir[0]\n",
    "\n",
    "                for m in _module_dir[1:]:\n",
    "                    _module_path = _module_path + '.' + m\n",
    "                print(_module_path)\n",
    "                plg_lib = importlib.import_module(_module_path)\n",
    "            else:\n",
    "                # import dir is the dirpath for the config file\n",
    "                _module_dir = os.path.dirname(args.config)\n",
    "                _module_dir = _module_dir.split('/')\n",
    "                _module_path = _module_dir[0]\n",
    "                for m in _module_dir[1:]:\n",
    "                    _module_path = _module_path + '.' + m\n",
    "                print(_module_path)\n",
    "                plg_lib = importlib.import_module(_module_path)\n",
    "\n",
    "    # set cudnn_benchmark\n",
    "    if cfg.get('cudnn_benchmark', False):\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    cfg.model.pretrained = None\n",
    "\n",
    "    # in case the test dataset is concatenated\n",
    "    samples_per_gpu = 1\n",
    "    if isinstance(cfg.data.test, dict):\n",
    "        cfg.data.test.test_mode = True\n",
    "        samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)\n",
    "        if samples_per_gpu > 1:\n",
    "            # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "            cfg.data.test.pipeline = replace_ImageToTensor(\n",
    "                cfg.data.test.pipeline)\n",
    "    elif isinstance(cfg.data.test, list):\n",
    "        for ds_cfg in cfg.data.test:\n",
    "            ds_cfg.test_mode = True\n",
    "        samples_per_gpu = max(\n",
    "            [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])\n",
    "        if samples_per_gpu > 1:\n",
    "            for ds_cfg in cfg.data.test:\n",
    "                ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "    # build the model and load checkpoint\n",
    "    cfg.model.train_cfg = None\n",
    "    model = build_model(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
    "    fp16_cfg = cfg.get('fp16', None)\n",
    "    if fp16_cfg is not None:\n",
    "        wrap_fp16_model(model)\n",
    "\n",
    "    # 1 MMDet3D APIs - inference.py version checkpoint load\n",
    "    if checkpoint is not None:\n",
    "        checkpoint = load_checkpoint(model, checkpoint, map_location='cpu')\n",
    "        if 'CLASSES' in checkpoint['meta']:\n",
    "            model.CLASSES = checkpoint['meta']['CLASSES']\n",
    "        else:\n",
    "            model.CLASSES = config.class_names\n",
    "        if 'PALETTE' in checkpoint['meta']:  # 3D Segmentor\n",
    "            model.PALETTE = checkpoint['meta']['PALETTE']\n",
    "\n",
    "    model.cfg = config\n",
    "\n",
    "    if device is not 'cpu':\n",
    "        torch.cuda.set_device(device)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    return model, cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/jeholee/omni3D/bev_based/BEVFormer/projects/mmdet3d_plugin/bevformer/modules/custom_base_transformer_layer.py:94: UserWarning: The arguments `feedforward_channels` in BaseTransformerLayer has been deprecated, now you should set `feedforward_channels` and other FFN related arguments to a dict named `ffn_cfgs`. \n",
      "  warnings.warn(\n",
      "/data/home/jeholee/omni3D/bev_based/BEVFormer/projects/mmdet3d_plugin/bevformer/modules/custom_base_transformer_layer.py:94: UserWarning: The arguments `ffn_dropout` in BaseTransformerLayer has been deprecated, now you should set `ffn_drop` and other FFN related arguments to a dict named `ffn_cfgs`. \n",
      "  warnings.warn(\n",
      "/data/home/jeholee/omni3D/bev_based/BEVFormer/projects/mmdet3d_plugin/bevformer/modules/custom_base_transformer_layer.py:94: UserWarning: The arguments `ffn_num_fcs` in BaseTransformerLayer has been deprecated, now you should set `num_fcs` and other FFN related arguments to a dict named `ffn_cfgs`. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plugin import\n",
      "projects.mmdet3d_plugin\n",
      "load checkpoint from local path: ../BEVFormer/ckpts/bevformer_r101_dcn_24ep.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-05 20:31:34,344 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.0.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,350 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.1.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,355 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.2.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,360 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.3.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,366 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.4.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,370 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.5.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,375 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.6.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,380 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.7.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,392 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.8.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,401 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.9.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,409 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.10.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,417 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.11.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,424 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.12.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,431 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.13.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,438 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.14.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,445 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.15.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,470 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.16.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,483 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.17.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,496 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.18.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,503 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.19.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,508 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.20.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,515 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.21.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,521 - root - INFO - ModulatedDeformConvPack img_backbone.layer3.22.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,528 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.0.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,539 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.1.conv2 is upgraded to version 2.\n",
      "2022-10-05 20:31:34,547 - root - INFO - ModulatedDeformConvPack img_backbone.layer4.2.conv2 is upgraded to version 2.\n"
     ]
    }
   ],
   "source": [
    "config = './projects/configs/bevformer/bevformer_base.py'\n",
    "checkpoint = '../BEVFormer/ckpts/bevformer_r101_dcn_24ep.pth'\n",
    "\n",
    "device = 'cuda:7'\n",
    "\n",
    "model, cfg = init_model(config, checkpoint, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using DataLoader (Jeho's custom pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_gpu = 1\n",
    "if isinstance(cfg.data.test, dict):\n",
    "    cfg.data.test.test_mode = True\n",
    "    samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)\n",
    "    if samples_per_gpu > 1:\n",
    "        # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "        cfg.data.test.pipeline = replace_ImageToTensor(\n",
    "            cfg.data.test.pipeline)\n",
    "elif isinstance(cfg.data.test, list):\n",
    "    for ds_cfg in cfg.data.test:\n",
    "        ds_cfg.test_mode = True\n",
    "    samples_per_gpu = max(\n",
    "        [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])\n",
    "    if samples_per_gpu > 1:\n",
    "        for ds_cfg in cfg.data.test:\n",
    "            ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "\n",
    "# distributed = True\n",
    "# init_dist(\"pytorch\", **cfg.dist_params)\n",
    "\n",
    "# build the dataloader\n",
    "dataset = build_dataset(cfg.data.test)\n",
    "\n",
    "data_loader = build_dataloader(\n",
    "    dataset,\n",
    "    samples_per_gpu=samples_per_gpu,\n",
    "    workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "    dist=False,\n",
    "    shuffle=False,\n",
    "    nonshuffler_sampler=cfg.data.nonshuffler_sampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING!!!!, Only can be used for obtain inference speed!!!!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                              ] 13/6019, 0.3 task/s, elapsed: 51s, ETA: 23550s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m have_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     15\u001b[0m         \n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# JEHO: Move image Tensor to CUDA device\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_metas\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg_metas\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdata\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1207\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1207\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1210\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1173\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1173\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1174\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1175\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1011\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1013\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1014\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/omnicv/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bbox_results = []\n",
    "mask_results = []\n",
    "dataset = data_loader.dataset\n",
    "rank, world_size = get_dist_info()\n",
    "if rank == 0:\n",
    "    prog_bar = mmcv.ProgressBar(len(dataset))\n",
    "time.sleep(2)  # This line can prevent deadlock problem in some cases.\n",
    "have_mask = False\n",
    "\n",
    "# device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for i, data in enumerate(data_loader):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # JEHO: Move image Tensor to CUDA device\n",
    "        data['img_metas'] = data['img_metas'][0].data\n",
    "        data['img'] = data['img'][0].data\n",
    "        data['img'][0] = data['img'][0].to(device)\n",
    "        \n",
    "        result = model(return_loss=False, rescale=True, **data)\n",
    "        # encode mask results\n",
    "        if isinstance(result, dict):\n",
    "            if 'bbox_results' in result.keys():\n",
    "                bbox_result = result['bbox_results']\n",
    "                batch_size = len(result['bbox_results'])\n",
    "                bbox_results.extend(bbox_result)\n",
    "            if 'mask_results' in result.keys() and result['mask_results'] is not None:\n",
    "                mask_result = custom_encode_mask_results(result['mask_results'])\n",
    "                mask_results.extend(mask_result)\n",
    "                have_mask = True\n",
    "        else:\n",
    "            batch_size = len(result)\n",
    "            bbox_results.extend(result)\n",
    "    \n",
    "    if rank == 0:\n",
    "        for _ in range(batch_size * world_size):\n",
    "            prog_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(data_loader):\n",
    "    with torch.no_grad():\n",
    "        if i == 0:\n",
    "            first_data = data\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[-37.5300, -37.5300, -37.5300,  ...,  81.4700,  80.4700,\n",
       "             76.4700],\n",
       "           [-40.5300, -40.5300, -41.5300,  ...,  83.4700,  81.4700,\n",
       "             77.4700],\n",
       "           [-41.5300, -41.5300, -41.5300,  ...,  81.4700,  77.4700,\n",
       "             72.4700],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]],\n",
       "\n",
       "          [[-54.2800, -54.2800, -54.2800,  ...,  64.7200,  63.7200,\n",
       "             59.7200],\n",
       "           [-57.2800, -57.2800, -58.2800,  ...,  66.7200,  64.7200,\n",
       "             60.7200],\n",
       "           [-55.2800, -55.2800, -55.2800,  ...,  64.7200,  60.7200,\n",
       "             55.7200],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]],\n",
       "\n",
       "          [[-62.6750, -62.6750, -62.6750,  ...,  52.3250,  51.3250,\n",
       "             47.3250],\n",
       "           [-65.6750, -65.6750, -66.6750,  ...,  54.3250,  52.3250,\n",
       "             48.3250],\n",
       "           [-66.6750, -66.6750, -66.6750,  ...,  52.3250,  48.3250,\n",
       "             43.3250],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]]],\n",
       "\n",
       "\n",
       "         [[[104.4700, 108.4700, 116.4700,  ...,  96.4700,  96.4700,\n",
       "             96.4700],\n",
       "           [129.4700, 130.4700, 132.4700,  ...,  96.4700,  96.4700,\n",
       "             96.4700],\n",
       "           [145.4700, 143.4700, 140.4700,  ...,  96.4700,  95.4700,\n",
       "             95.4700],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]],\n",
       "\n",
       "          [[ 82.7200,  86.7200,  94.7200,  ...,  78.7200,  78.7200,\n",
       "             78.7200],\n",
       "           [107.7200, 108.7200, 110.7200,  ...,  78.7200,  78.7200,\n",
       "             78.7200],\n",
       "           [123.7200, 121.7200, 118.7200,  ...,  78.7200,  77.7200,\n",
       "             77.7200],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]],\n",
       "\n",
       "          [[ 72.3250,  76.3250,  84.3250,  ...,  68.3250,  68.3250,\n",
       "             68.3250],\n",
       "           [ 97.3250,  98.3250, 100.3250,  ...,  68.3250,  68.3250,\n",
       "             68.3250],\n",
       "           [113.3250, 111.3250, 108.3250,  ...,  68.3250,  67.3250,\n",
       "             67.3250],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]]],\n",
       "\n",
       "\n",
       "         [[[-36.5300, -39.5300, -41.5300,  ..., 148.4700, 148.4700,\n",
       "            148.4700],\n",
       "           [-30.5300, -33.5300, -35.5300,  ..., 149.4700, 149.4700,\n",
       "            149.4700],\n",
       "           [-26.5300, -29.5300, -33.5300,  ..., 151.4700, 151.4700,\n",
       "            151.4700],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]],\n",
       "\n",
       "          [[-49.2800, -52.2800, -57.2800,  ..., 130.7200, 130.7200,\n",
       "            130.7200],\n",
       "           [-43.2800, -46.2800, -51.2800,  ..., 131.7200, 131.7200,\n",
       "            131.7200],\n",
       "           [-39.2800, -42.2800, -46.2800,  ..., 133.7200, 133.7200,\n",
       "            133.7200],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]],\n",
       "\n",
       "          [[-62.6750, -65.6750, -69.6750,  ..., 120.3250, 120.3250,\n",
       "            120.3250],\n",
       "           [-56.6750, -59.6750, -63.6750,  ..., 121.3250, 121.3250,\n",
       "            121.3250],\n",
       "           [-52.6750, -55.6750, -59.6750,  ..., 123.3250, 123.3250,\n",
       "            123.3250],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]]],\n",
       "\n",
       "\n",
       "         [[[116.4700, 115.4700, 114.4700,  ..., -43.5300, -43.5300,\n",
       "            -43.5300],\n",
       "           [116.4700, 115.4700, 115.4700,  ..., -43.5300, -43.5300,\n",
       "            -43.5300],\n",
       "           [116.4700, 116.4700, 115.4700,  ..., -44.5300, -44.5300,\n",
       "            -45.5300],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]],\n",
       "\n",
       "          [[100.7200,  99.7200,  98.7200,  ..., -57.2800, -57.2800,\n",
       "            -57.2800],\n",
       "           [100.7200,  99.7200,  99.7200,  ..., -57.2800, -57.2800,\n",
       "            -57.2800],\n",
       "           [100.7200, 100.7200,  99.7200,  ..., -58.2800, -58.2800,\n",
       "            -59.2800],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]],\n",
       "\n",
       "          [[ 95.3250,  94.3250,  93.3250,  ..., -68.6750, -68.6750,\n",
       "            -68.6750],\n",
       "           [ 95.3250,  94.3250,  94.3250,  ..., -68.6750, -68.6750,\n",
       "            -68.6750],\n",
       "           [ 95.3250,  95.3250,  94.3250,  ..., -69.6750, -69.6750,\n",
       "            -70.6750],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]]],\n",
       "\n",
       "\n",
       "         [[[-19.5300, -17.5300, -14.5300,  ..., -36.5300, -35.5300,\n",
       "            -34.5300],\n",
       "           [ -1.5300,  -0.5300,   0.4700,  ..., -31.5300, -30.5300,\n",
       "            -30.5300],\n",
       "           [  3.4700,   3.4700,   3.4700,  ..., -24.5300, -24.5300,\n",
       "            -24.5300],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]],\n",
       "\n",
       "          [[-44.2800, -42.2800, -39.2800,  ..., -60.2800, -59.2800,\n",
       "            -58.2800],\n",
       "           [-26.2800, -25.2800, -24.2800,  ..., -55.2800, -54.2800,\n",
       "            -54.2800],\n",
       "           [-21.2800, -21.2800, -21.2800,  ..., -46.2800, -46.2800,\n",
       "            -46.2800],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]],\n",
       "\n",
       "          [[-57.6750, -55.6750, -52.6750,  ..., -71.6750, -70.6750,\n",
       "            -69.6750],\n",
       "           [-39.6750, -38.6750, -37.6750,  ..., -66.6750, -65.6750,\n",
       "            -65.6750],\n",
       "           [-34.6750, -34.6750, -34.6750,  ..., -57.6750, -57.6750,\n",
       "            -57.6750],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]]],\n",
       "\n",
       "\n",
       "         [[[-45.5300, -44.5300, -44.5300,  ...,  96.4700,  96.4700,\n",
       "             96.4700],\n",
       "           [-44.5300, -44.5300, -43.5300,  ...,  96.4700,  96.4700,\n",
       "             96.4700],\n",
       "           [-45.5300, -44.5300, -43.5300,  ...,  96.4700,  96.4700,\n",
       "             96.4700],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]],\n",
       "\n",
       "          [[-61.2800, -60.2800, -60.2800,  ...,  81.7200,  81.7200,\n",
       "             81.7200],\n",
       "           [-60.2800, -60.2800, -59.2800,  ...,  81.7200,  81.7200,\n",
       "             81.7200],\n",
       "           [-61.2800, -60.2800, -59.2800,  ...,  81.7200,  81.7200,\n",
       "             81.7200],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]],\n",
       "\n",
       "          [[-73.6750, -72.6750, -72.6750,  ...,  73.3250,  73.3250,\n",
       "             73.3250],\n",
       "           [-72.6750, -72.6750, -71.6750,  ...,  73.3250,  73.3250,\n",
       "             73.3250],\n",
       "           [-73.6750, -72.6750, -71.6750,  ...,  73.3250,  73.3250,\n",
       "             73.3250],\n",
       "           ...,\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000],\n",
       "           [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,\n",
       "              0.0000]]]]], device='cuda:7')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_data['img'][0].data[0].to('cuda:7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['img'] = data['img'][0].data[0].to('cuda:7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_metas[0].data[0][0]['scene_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_metas_tmp = img_metas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_metas_tmp = img_metas_tmp.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_metas_tmp[0][0]['scene_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Custom Data feeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"inference.py\"\"\"\n",
    "\n",
    "device = next(model.parameters()).device  # model device\n",
    "\n",
    "# build the data pipeline\n",
    "test_pipeline = deepcopy(cfg.data.test.pipeline)\n",
    "test_pipeline = Compose(test_pipeline)\n",
    "box_type_3d, box_mode_3d = get_box_type(cfg.data.test.box_type_3d)\n",
    "\n",
    "ann_file = \"./data/nuscenes/nuscenes_infos_temporal_val.pkl\"\n",
    "\n",
    "# get data info containing calib\n",
    "data_infos = mmcv.load(ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_infos['infos'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the info corresponding to this image\n",
    "for x in data_infos['images']:\n",
    "    if osp.basename(x['file_name']) != osp.basename(image):\n",
    "        continue\n",
    "    img_info = x\n",
    "    break\n",
    "data = dict(\n",
    "    img_prefix=osp.dirname(image),\n",
    "    img_info=dict(filename=osp.basename(image)),\n",
    "    box_type_3d=box_type_3d,\n",
    "    box_mode_3d=box_mode_3d,\n",
    "    img_fields=[],\n",
    "    bbox3d_fields=[],\n",
    "    pts_mask_fields=[],\n",
    "    pts_seg_fields=[],\n",
    "    bbox_fields=[],\n",
    "    mask_fields=[],\n",
    "    seg_fields=[])\n",
    "\n",
    "# camera points to image conversion\n",
    "if box_mode_3d == Box3DMode.CAM:\n",
    "    data['img_info'].update(dict(cam_intrinsic=img_info['cam_intrinsic']))\n",
    "\n",
    "data = test_pipeline(data)\n",
    "\n",
    "data = collate([data], samples_per_gpu=1)\n",
    "if next(model.parameters()).is_cuda:\n",
    "    # scatter to specified GPU\n",
    "    data = scatter(data, [device.index])[0]\n",
    "else:\n",
    "    # this is a workaround to avoid the bug of MMDataParallel\n",
    "    data['img_metas'] = data['img_metas'][0].data\n",
    "    data['img'] = data['img'][0].data\n",
    "\n",
    "# forward the model\n",
    "with torch.no_grad():\n",
    "    result = model(return_loss=False, rescale=True, **data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTangentPatches(erp_img, tangent_h, tangent_w, num_rows, num_cols, phi_centers, fov):\n",
    "    [erp_h, erp_w, _] = erp_img.shape\n",
    "    img_new = erp_img.astype(np.float32) / 255\n",
    "    img_new = np.transpose(img_new, [2, 0, 1]) # permutation, 세 번째 axis가 첫 번째 axis로\n",
    "    img_new = torch.from_numpy(img_new) # Create Tensor from numpy array\n",
    "    img_new = img_new.unsqueeze(0) # Increase Tensor dimension by 1\n",
    "    \n",
    "    height, width = tangent_h, tangent_w\n",
    "\n",
    "    FOV = fov\n",
    "    FOV = [FOV[0] / 360.0, FOV[1] / 180.0]\n",
    "    FOV = torch.tensor(FOV, dtype=torch.float32)\n",
    "\n",
    "    PI = math.pi\n",
    "    PI_2 = math.pi * 0.5\n",
    "    PI2 = math.pi * 2\n",
    "\n",
    "    yy, xx = torch.meshgrid(torch.linspace(0, 1, height), torch.linspace(0, 1, width))\n",
    "    screen_points = torch.stack([xx.flatten(), yy.flatten()], -1)\n",
    "    \n",
    "    num_rows = num_rows\n",
    "    num_cols = num_cols\n",
    "    phi_centers = phi_centers\n",
    "\n",
    "    phi_interval = 180 // num_rows # 45도\n",
    "    all_combos = []\n",
    "    erp_mask = []\n",
    "    \n",
    "    for i, n_cols in enumerate(num_cols):\n",
    "        for j in np.arange(n_cols): # 0 ~ num_cols.length\n",
    "            theta_interval = 360 / n_cols # 현재 row (위도)에서 쪼개질 경도 (col)의 위치\n",
    "            theta_center = j * theta_interval + theta_interval / 2\n",
    "            center = [theta_center, phi_centers[i]] # 각 tangent image의 center position\n",
    "            all_combos.append(center)\n",
    "\n",
    "            # 구좌표계에서의 tangent image가 차지하는 영역에 대한 좌표들\n",
    "            up = phi_centers[i] + phi_interval / 2\n",
    "            down = phi_centers[i] - phi_interval / 2\n",
    "            left = theta_center - theta_interval / 2\n",
    "            right = theta_center + theta_interval / 2\n",
    "\n",
    "            # ERP image에서 현재 tangent가 차지하는 영역에 대한 pixel 위치들\n",
    "            up = int((up + 90) / 180 * erp_h)\n",
    "            down = int((down + 90) / 180 * erp_h)\n",
    "            left = int(left / 360 * erp_w)\n",
    "            right = int(right / 360 * erp_w)\n",
    "\n",
    "            # ERP 이미지에서 현재 tangent image 영역에 해당하는 부분에 1로 마스킹\n",
    "            mask = np.zeros((erp_h, erp_w), dtype=int)\n",
    "            mask[down:up, left:right] = 1\n",
    "            erp_mask.append(mask)\n",
    "\n",
    "    all_combos = np.vstack(all_combos)\n",
    "    shifts = np.arange(all_combos.shape[0]) * width\n",
    "    shifts = torch.from_numpy(shifts).float()\n",
    "    erp_mask = np.stack(erp_mask)\n",
    "    erp_mask = torch.from_numpy(erp_mask).float()\n",
    "    n_patch = all_combos.shape[0]\n",
    "    \n",
    "    center_point = torch.from_numpy(all_combos).float()  # -180 to 180, -90 to 90\n",
    "    center_point[:, 0] = (center_point[:, 0]) / 360  #0 to 1\n",
    "    center_point[:, 1] = (center_point[:, 1] + 90) / 180  #0 to 1\n",
    "\n",
    "    cp = center_point * 2 - 1\n",
    "    cp[:, 0] = cp[:, 0] * PI\n",
    "    cp[:, 1] = cp[:, 1] * PI_2\n",
    "    cp = cp.unsqueeze(1)\n",
    "\n",
    "    convertedCoord = screen_points * 2 - 1\n",
    "    convertedCoord[:, 0] = convertedCoord[:, 0] * PI\n",
    "    convertedCoord[:, 1] = convertedCoord[:, 1] * PI_2\n",
    "    convertedCoord = convertedCoord * (torch.ones(screen_points.shape, dtype=torch.float32) * FOV)\n",
    "    convertedCoord = convertedCoord.unsqueeze(0).repeat(cp.shape[0], 1, 1)\n",
    "    \n",
    "    x = convertedCoord[:, :, 0]\n",
    "    y = convertedCoord[:, :, 1]\n",
    "\n",
    "    rou = torch.sqrt(x ** 2 + y ** 2)\n",
    "    c = torch.atan(rou)\n",
    "    sin_c = torch.sin(c)\n",
    "    cos_c = torch.cos(c)\n",
    "    lat = torch.asin(cos_c * torch.sin(cp[:, :, 1]) + (y * sin_c * torch.cos(cp[:, :, 1])) / rou)\n",
    "    lon = cp[:, :, 0] + torch.atan2(x * sin_c, rou * torch.cos(cp[:, :, 1]) * cos_c - y * torch.sin(cp[:, :, 1]) * sin_c)\n",
    "    lat_new = lat / PI_2 \n",
    "    lon_new = lon / PI \n",
    "    lon_new[lon_new > 1] -= 2\n",
    "    lon_new[lon_new<-1] += 2\n",
    "\n",
    "    lon_new = lon_new.view(1, n_patch, height, width).permute(0, 2, 1, 3).contiguous().view(height, n_patch*width)\n",
    "    lat_new = lat_new.view(1, n_patch, height, width).permute(0, 2, 1, 3).contiguous().view(height, n_patch*width)\n",
    "    grid = torch.stack([lon_new, lat_new], -1)\n",
    "\n",
    "    grid = grid.unsqueeze(0)\n",
    "    persp = F.grid_sample(img_new, grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "\n",
    "    # persp_int = persp[0].permute(1, 2, 0).numpy()\n",
    "    # persp_int = persp_int * 255\n",
    "    # plt.figure(figsize=(20, 12))\n",
    "    # plt.imshow(persp_int[:,:,[2,1,0]].astype(np.uint8), aspect=1)\n",
    "    # plt.show()\n",
    "\n",
    "    persp_reshape = F.unfold(persp, kernel_size=(height, width), stride=(height, width))\n",
    "    persp_reshape = persp_reshape.reshape(1, 3, height, width, n_patch)\n",
    "\n",
    "    return persp_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. show the results\n",
    "def visualize_single_inference(result, data, score_thr=0.0, thickness=2):\n",
    "    # show_result_meshlab(data, result, output_dir, score_thr, show, snapshot, task='mono-det')\n",
    "    ################ show_proj_det_result_meshlab(data, result, output_dir, score_thr, show, snapshot) ################\n",
    "    \"\"\"Show result of projecting 3D bbox to 2D image by meshlab.\"\"\"\n",
    "    assert 'img' in data.keys(), 'image data is not provided for visualization'\n",
    "\n",
    "    img_filename = data['img_metas'][0][0]['filename']\n",
    "    file_name = osp.split(img_filename)[-1].split('.')[0]\n",
    "    show = True\n",
    "    snapshot = False\n",
    "\n",
    "    # read from file because img in data_dict has undergone pipeline transform\n",
    "    img = mmcv.imread(img_filename)\n",
    "\n",
    "    if 'pts_bbox' in result[0].keys():\n",
    "        result[0] = result[0]['pts_bbox']\n",
    "    elif 'img_bbox' in result[0].keys():\n",
    "        result[0] = result[0]['img_bbox']\n",
    "    pred_bboxes = result[0]['boxes_3d'].tensor.numpy()\n",
    "    pred_scores = result[0]['scores_3d'].numpy()\n",
    "\n",
    "    # filter out low score bboxes for visualization\n",
    "    if score_thr > 0:\n",
    "        inds = pred_scores > score_thr\n",
    "        pred_bboxes = pred_bboxes[inds]\n",
    "\n",
    "    box_mode = 'camera'\n",
    "    pred_bboxes = CameraInstance3DBoxes(pred_bboxes, box_dim=pred_bboxes.shape[-1], origin=(0.5, 1.0, 0.5))\n",
    "\n",
    "    ############################ show_multi_modality_result() ############################\n",
    "    proj_mat = data['img_metas'][0][0]['cam2img']\n",
    "    img_metas = None\n",
    "    pred_bbox_color=(241, 101, 72)\n",
    "\n",
    "    # result_path = osp.join(output_dir, file_name)\n",
    "    # mmcv.mkdir_or_exist(result_path)\n",
    "\n",
    "    show_img = img.copy()\n",
    "    proj_mat = copy.deepcopy(proj_mat)\n",
    "    corners_3d = pred_bboxes.corners\n",
    "    num_bbox = corners_3d.shape[0]\n",
    "    points_3d = corners_3d.reshape(-1, 3)\n",
    "\n",
    "    if not isinstance(proj_mat, torch.Tensor):\n",
    "        proj_mat = torch.from_numpy(np.array(proj_mat))\n",
    "\n",
    "    assert (proj_mat.shape == torch.Size([3, 3])\n",
    "            or proj_mat.shape == torch.Size([4, 4]))\n",
    "    proj_mat = proj_mat.float().cpu()\n",
    "\n",
    "    # project to 2d to get image coords (uv)\n",
    "    uv_origin = points_cam2img(points_3d, proj_mat)\n",
    "    uv_origin = (uv_origin - 1).round()\n",
    "    imgfov_pts_2d = uv_origin[..., :2].reshape(num_bbox, 8, 2).numpy()\n",
    "\n",
    "    ################ draw boundinb box ###################\n",
    "    line_indices = ((0, 1), (0, 3), (0, 4), (1, 2), (1, 5), (3, 2), (3, 7),\n",
    "                    (4, 5), (4, 7), (2, 6), (5, 6), (6, 7))\n",
    "    for i in range(num_bbox):\n",
    "        corners = imgfov_pts_2d[i].astype(np.int)\n",
    "        for start, end in line_indices:\n",
    "            cv2.line(show_img, (corners[start, 0], corners[start, 1]),\n",
    "                     (corners[end, 0], corners[end, 1]), pred_bbox_color, thickness,\n",
    "                     cv2.LINE_AA)\n",
    "\n",
    "    return show_img.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: Dual-fisheye images and Perspective image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Calibration\n",
    "1. perspective cam\n",
    "    - center point of the image: 696, 256\n",
    "    - calibrated center point: 687.158398, 317.752196\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_img_filename(l_name, r_name, p_name):\n",
    "    with open(\"./working_dir/360dataset/left_cam.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        data['images'][0]['file_name'] = l_name\n",
    "    with open(\"./working_dir/360dataset/left_cam.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "    with open(\"./working_dir/360dataset/right_cam.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        data['images'][0]['file_name'] = r_name\n",
    "    with open(\"./working_dir/360dataset/right_cam.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "        \n",
    "    with open(\"./working_dir/360dataset/pers_cam.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        data['images'][0]['file_name'] = p_name\n",
    "    with open(\"./working_dir/360dataset/pers_cam.json\", \"w\") as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_sample_from_dataset(fname=None):\n",
    "    erp_dir = \"./working_dir/360dataset/image_erp/\"\n",
    "    pers_dir = \"./working_dir/360dataset/perspective_images/data_2d_raw/\"\n",
    "    img_files = os.listdir(erp_dir)\n",
    "    \n",
    "    if fname is None:\n",
    "        fname = img_files[randrange(len(img_files))] # randomly pick\n",
    "\n",
    "    print(fname)\n",
    "    \n",
    "    l_cam = \"./working_dir/360dataset/left_cam.json\"\n",
    "    r_cam = \"./working_dir/360dataset/right_cam.json\"\n",
    "    p_cam = \"./working_dir/360dataset/pers_cam.json\"\n",
    "\n",
    "    erp_img = erp_dir + fname\n",
    "    img = cv2.imread(erp_img, cv2.IMREAD_COLOR)\n",
    "    # plt.imshow(img[:,:,[2,1,0]])\n",
    "\n",
    "    width = img.shape[1]\n",
    "    width_cutoff = width // 2\n",
    "    s1 = img[:, :width_cutoff]\n",
    "    s2 = img[:, width_cutoff:]\n",
    "    l_img = \"./working_dir/360dataset/l_sample.png\"\n",
    "    r_img = \"./working_dir/360dataset/r_sample.png\"\n",
    "    cv2.imwrite(l_img, s1)\n",
    "    cv2.imwrite(r_img, s2)\n",
    "    p_img = pers_dir + fname\n",
    "    \n",
    "    change_img_filename(l_img, r_img, p_img)\n",
    "    \n",
    "    return erp_img, l_img, r_img, p_img, l_cam, r_cam, p_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_and_vis(num_sample=1, fname=None, score_thr=0.0, thickness=1):\n",
    "    for i in range(num_sample):\n",
    "        erp_img, l_img, r_img, p_img, l_cam, r_cam, p_cam = select_sample_from_dataset(fname)\n",
    "\n",
    "        # Inference on dfisheye and perspective\n",
    "        t1 = time.time()\n",
    "        l_output = inference_mono_3d_detector(model, l_img, l_cam)\n",
    "        r_output = inference_mono_3d_detector(model, r_img, r_cam)\n",
    "        print(\"dfisheye latency: \" + str((time.time() - t1) * 1000))\n",
    "        \n",
    "        t1 = time.time()\n",
    "        p_output = inference_mono_3d_detector(model, p_img, p_cam)\n",
    "        print(\"perspective latency: \" + str((time.time() - t1) * 1000))\n",
    "        \n",
    "        # Visualize results\n",
    "        l_resimg = visualize_single_inference(l_output[0], l_output[1], score_thr=score_thr, thickness=thickness)\n",
    "        r_resimg = visualize_single_inference(r_output[0], r_output[1], score_thr=score_thr, thickness=thickness)\n",
    "        concat_img = cv2.hconcat([l_resimg, r_resimg])\n",
    "        \n",
    "        p_resimg = visualize_single_inference(p_output[0], p_output[1], score_thr=score_thr, thickness=thickness)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(concat_img[:,:,[2,1,0]].astype(np.uint8), aspect=1)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(p_resimg[:,:,[2,1,0]].astype(np.uint8), aspect=1)\n",
    "        \n",
    "        # Generate Tangent Images\n",
    "        \n",
    "        # height, width = 512, 512\n",
    "        # height, width = 96, 96\n",
    "        # height, width = 256, 256\n",
    "        # num_rows = 4\n",
    "        # num_cols = [3, 6, 6, 3]\n",
    "        # phi_centers = [-67.5, -22.5, 22.5, 67.5]\n",
    "        # num_rows = 2\n",
    "        # num_cols = [6, 6]\n",
    "        # phi_centers = [-22.5, 22.5]\n",
    "        # num_rows = 5\n",
    "        # num_cols = [3, 6, 8, 6, 3]\n",
    "        # phi_centers = [-72.2, -36.1, 0, 36.1, 72.2]\n",
    "        # num_rows = 6\n",
    "        # num_cols = [3, 8, 12, 12, 8, 3]\n",
    "        # phi_centers = [-75.2, -45.93, -15.72, 15.72, 45.93, 75.2]\n",
    "        tangent_h = 512\n",
    "        tangent_w = 512\n",
    "        num_rows = 2\n",
    "        num_cols = [6, 6]\n",
    "        phi_centers = [-22.5, 22.5]\n",
    "        \n",
    "        img = cv2.imread(erp_img, cv2.IMREAD_COLOR)\n",
    "        patches = genTangentPatches(img, tangent_h, tangent_w, num_rows, num_cols, phi_centers)\n",
    "        \n",
    "        # Inference on tangent images\n",
    "        patch_num = 0\n",
    "        for num_col in num_cols:\n",
    "            patch_num = num_col + patch_num\n",
    "        \n",
    "        time_sum = 0\n",
    "        for i in range(patch_num):\n",
    "            cur_patch = patches[0, :, :, :, i].permute(1, 2, 0).numpy()\n",
    "            cur_patch = cur_patch * 255\n",
    "            t_img = \"./working_dir/360dataset/tangent_patch.png\"\n",
    "            cv2.imwrite(t_img, cur_patch)\n",
    "            t_cam = \"./working_dir/360dataset/tangent_patch.json\"\n",
    "            \n",
    "            # Single tangent image inference\n",
    "            t1 = time.time()\n",
    "            t_output = inference_mono_3d_detector(model, t_img, t_cam)\n",
    "            latency = (time.time() - t1) * 1000\n",
    "            time_sum = time_sum + latency\n",
    "\n",
    "            t_resimg = visualize_single_inference(t_output[0], t_output[1], score_thr=score_thr, thickness=thickness)\n",
    "            \n",
    "            plt.figure(figsize=(7, 7))\n",
    "            plt.imshow(t_resimg[:,:,[2,1,0]].astype(np.uint8), aspect=1)\n",
    "\n",
    "        plt.show()\n",
    "        print(\"tangent patch avg. latency: \" + str(time_sum / patch_num))\n",
    "        \n",
    "        # Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inference_and_vis(1, \"0000004123.png\", score_thr=0.15, thickness=2)\n",
    "\n",
    "# 0000008119.png\n",
    "# 0000005572.png\n",
    "# 0000011342.png\n",
    "# 0000006907.png\n",
    "# 0000004123.png\n",
    "\n",
    "inference_and_vis(3, score_thr=0.15, thickness=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insta 360 One X2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_select_sample_from_instaonex2_dataset(fname=None):\n",
    "    img_dir = \"./working_dir/360_urban_scene_instaOneX2/data_rgb/\"\n",
    "    img_files = os.listdir(img_dir)\n",
    "\n",
    "    if fname is None:\n",
    "        fname = img_files[randrange(len(img_files))] # randomly pick\n",
    "\n",
    "    print(fname)\n",
    "    erp_img = img_dir + fname\n",
    "    cam = \"./working_dir/360_urban_scene_instaOneX2/onex2_cam.json\"\n",
    "\n",
    "    # img = cv2.imread(erp_img, cv2.IMREAD_COLOR)\n",
    "    # width = img.shape[1]\n",
    "    # width_cutoff = width // 2\n",
    "    # s1 = img[:, :width_cutoff]\n",
    "    # s2 = img[:, width_cutoff:]\n",
    "    # l_img = \"./working_dir/360dataset/l_sample.png\"\n",
    "    # r_img = \"./working_dir/360dataset/r_sample.png\"\n",
    "    # cv2.imwrite(l_img, s1)\n",
    "    # cv2.imwrite(r_img, s2)\n",
    "    # p_img = pers_dir + fname\n",
    "\n",
    "    with open(\"./working_dir/360_urban_scene_instaOneX2/onex2_cam.json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        data['images'][0]['file_name'] = erp_img\n",
    "    with open(\"./working_dir/360_urban_scene_instaOneX2/onex2_cam.json\", \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "    \n",
    "    return erp_img, cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_and_vis_instaonex2(num_sample=1, fname=None, score_thr=0.0, thickness=1):\n",
    "    for i in range(num_sample):\n",
    "        erp_img, cam = select_select_sample_from_instaonex2_dataset(fname)\n",
    "\n",
    "        # ERP image\n",
    "        t1 = time.time()\n",
    "        erp_output = inference_mono_3d_detector(model, erp_img, cam)\n",
    "        print(\"erp latency: \" + str((time.time() - t1) * 1000))\n",
    "        \n",
    "        erp_resimg = visualize_single_inference(erp_output[0], erp_output[1], score_thr=score_thr, thickness=thickness)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(erp_resimg[:,:,[2,1,0]].astype(np.uint8), aspect=1)\n",
    "        \n",
    "        # 3840x2160\n",
    "        # Tangent Images\n",
    "\n",
    "        # tangent_h = 512\n",
    "        # tangent_w = 512\n",
    "        tangent_h = 1000\n",
    "        tangent_w = 1000\n",
    "        # num_rows = 2\n",
    "        # num_cols = [6, 6]\n",
    "        # phi_centers = [-22.5, 22.5]\n",
    "        num_rows = 1\n",
    "        num_cols = [6]\n",
    "        phi_centers = [0]\n",
    "        # fov  = [90, 90]\n",
    "        fov = [60, 60]\n",
    "\n",
    "        img = cv2.imread(erp_img, cv2.IMREAD_COLOR)\n",
    "        patches = genTangentPatches(img, tangent_h, tangent_w, num_rows, num_cols, phi_centers, fov)\n",
    "        \n",
    "        # Inference on tangent images\n",
    "        patch_num = 0\n",
    "        for num_col in num_cols:\n",
    "            patch_num = num_col + patch_num\n",
    "        \n",
    "        time_sum = 0\n",
    "        for i in range(patch_num):\n",
    "            cur_patch = patches[0, :, :, :, i].permute(1, 2, 0).numpy()\n",
    "            cur_patch = cur_patch * 255\n",
    "            t_img = \"./working_dir/360_urban_scene_instaOneX2/tangent_patch.png\"\n",
    "            cv2.imwrite(t_img, cur_patch)\n",
    "            t_cam = \"./working_dir/360_urban_scene_instaOneX2/tangent_patch.json\"\n",
    "            \n",
    "            # Single tangent image inference\n",
    "            t1 = time.time()\n",
    "            t_output = inference_mono_3d_detector(model, t_img, t_cam)\n",
    "            latency = (time.time() - t1) * 1000\n",
    "            time_sum = time_sum + latency\n",
    "\n",
    "            t_resimg = visualize_single_inference(t_output[0], t_output[1], score_thr=score_thr, thickness=thickness)\n",
    "            \n",
    "            plt.figure(figsize=(7, 7))\n",
    "            plt.imshow(t_resimg[:,:,[2,1,0]].astype(np.uint8), aspect=1)\n",
    "\n",
    "        plt.show()\n",
    "        print(\"tangent patch avg. latency: \" + str(time_sum / patch_num))\n",
    "        \n",
    "        # Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_and_vis_instaonex2(3, score_thr=0.15, thickness=3)\n",
    "inference_and_vis_instaonex2(1, \"0000779.png\", score_thr=0.1, thickness=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f7588f70aea7a293617952afad28af0f56660e2a335e66f4f00c4280b8a78806"
  },
  "kernelspec": {
   "display_name": "omnicv",
   "language": "python",
   "name": "omnicv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
